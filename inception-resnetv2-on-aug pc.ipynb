{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-19T00:04:51.084640Z","iopub.status.busy":"2023-09-19T00:04:51.084226Z","iopub.status.idle":"2023-09-19T00:04:55.446473Z","shell.execute_reply":"2023-09-19T00:04:55.445453Z","shell.execute_reply.started":"2023-09-19T00:04:51.084602Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img, img_to_array, array_to_img\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, auc, roc_curve\n","from tensorflow.keras.applications import InceptionResNetV2  # Use Inception-ResNetV2\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-19T00:04:55.449384Z","iopub.status.busy":"2023-09-19T00:04:55.448654Z","iopub.status.idle":"2023-09-19T00:04:56.612608Z","shell.execute_reply":"2023-09-19T00:04:56.611276Z","shell.execute_reply.started":"2023-09-19T00:04:55.449347Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","[]\n"]}],"source":["# Check GPU availability\n","import tensorflow as tf\n","print(tf.test.is_gpu_available())\n","print(tf.config.list_physical_devices('GPU'))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-19T00:04:56.614929Z","iopub.status.busy":"2023-09-19T00:04:56.614471Z","iopub.status.idle":"2023-09-19T00:04:56.622859Z","shell.execute_reply":"2023-09-19T00:04:56.622038Z","shell.execute_reply.started":"2023-09-19T00:04:56.614883Z"},"trusted":true},"outputs":[],"source":["# Load the dataset and visualize images\n","labels = os.listdir(\"C:\\\\Users\\\\cbz\\\\Downloads\\\\train\")\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-19T00:04:56.627104Z","iopub.status.busy":"2023-09-19T00:04:56.625713Z","iopub.status.idle":"2023-09-19T00:04:56.633256Z","shell.execute_reply":"2023-09-19T00:04:56.632342Z","shell.execute_reply.started":"2023-09-19T00:04:56.627074Z"},"trusted":true},"outputs":[],"source":["# Load the data using ImageDataGenerator\n","datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    rotation_range=30\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-19T00:04:56.638819Z","iopub.status.busy":"2023-09-19T00:04:56.637905Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 11600 images belonging to 4 classes.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\cbz\\Downloads\\inception-resnetv2-on-aug pc.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     directory,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     target_size\u001b[39m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x, y \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cbz/Downloads/inception-resnetv2-on-aug%20pc.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Encode the labels using one-hot encoding\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\cbz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     index_array \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_generator)\n\u001b[0;32m    166\u001b[0m \u001b[39m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_batches_of_transformed_samples(index_array)\n","File \u001b[1;32mc:\\Users\\cbz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:384\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator:\n\u001b[0;32m    383\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mget_random_transform(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 384\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_data_generator\u001b[39m.\u001b[39;49mapply_transform(x, params)\n\u001b[0;32m    385\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mstandardize(x)\n\u001b[0;32m    386\u001b[0m batch_x[i] \u001b[39m=\u001b[39m x\n","File \u001b[1;32mc:\\Users\\cbz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:2011\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   2008\u001b[0m img_col_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2009\u001b[0m img_channel_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2011\u001b[0m x \u001b[39m=\u001b[39m apply_affine_transform(\n\u001b[0;32m   2012\u001b[0m     x,\n\u001b[0;32m   2013\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtheta\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2014\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2015\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mty\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2016\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mshear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2017\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2018\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2019\u001b[0m     row_axis\u001b[39m=\u001b[39;49mimg_row_axis,\n\u001b[0;32m   2020\u001b[0m     col_axis\u001b[39m=\u001b[39;49mimg_col_axis,\n\u001b[0;32m   2021\u001b[0m     channel_axis\u001b[39m=\u001b[39;49mimg_channel_axis,\n\u001b[0;32m   2022\u001b[0m     fill_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_mode,\n\u001b[0;32m   2023\u001b[0m     cval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcval,\n\u001b[0;32m   2024\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation_order,\n\u001b[0;32m   2025\u001b[0m )\n\u001b[0;32m   2027\u001b[0m \u001b[39mif\u001b[39;00m transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2028\u001b[0m     x \u001b[39m=\u001b[39m apply_channel_shift(\n\u001b[0;32m   2029\u001b[0m         x,\n\u001b[0;32m   2030\u001b[0m         transform_parameters[\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   2031\u001b[0m         img_channel_axis,\n\u001b[0;32m   2032\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\cbz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:2620\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2607\u001b[0m     final_offset \u001b[39m=\u001b[39m transform_matrix[:\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[0;32m   2609\u001b[0m     channel_images \u001b[39m=\u001b[39m [\n\u001b[0;32m   2610\u001b[0m         ndimage\u001b[39m.\u001b[39minterpolation\u001b[39m.\u001b[39maffine_transform(\n\u001b[0;32m   2611\u001b[0m             x_channel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2618\u001b[0m         \u001b[39mfor\u001b[39;00m x_channel \u001b[39min\u001b[39;00m x\n\u001b[0;32m   2619\u001b[0m     ]\n\u001b[1;32m-> 2620\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack(channel_images, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   2621\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrollaxis(x, \u001b[39m0\u001b[39m, channel_axis \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m   2622\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n","File \u001b[1;32mc:\\Users\\cbz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:433\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    431\u001b[0m sl \u001b[39m=\u001b[39m (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m),) \u001b[39m*\u001b[39m axis \u001b[39m+\u001b[39m (_nx\u001b[39m.\u001b[39mnewaxis,)\n\u001b[0;32m    432\u001b[0m expanded_arrays \u001b[39m=\u001b[39m [arr[sl] \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(expanded_arrays, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout)\n","File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Load the dataset and split into training and testing sets\n","directory = \"C:\\\\Users\\\\cbz\\\\Downloads\\\\train\"\n","target_size = (240, 240)\n","batch_size = 11600 # Adjust batch size as needed\n","\n","# Load the dataset and split into training and testing sets\n","data = datagen.flow_from_directory(\n","    directory,\n","    target_size=target_size,\n","    batch_size=batch_size,  # Specify batch size here\n","    class_mode='categorical',\n","    subset='training'\n",")\n","\n","# Split the data into training and testing sets\n","x, y = data.next()\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","# Encode the labels using one-hot encoding\n","label_bin = LabelBinarizer()\n","y_train = label_bin.fit_transform(y_train)\n","y_test = label_bin.transform(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create the Inception-ResNetV2 base model\n","base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(240, 240, 3))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Add custom layers for classification on top of the base model\n","model = Sequential([\n","    base_model,\n","    GlobalAveragePooling2D(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.5),\n","    Dense(4, activation='sigmoid')\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Freeze the layers in the base model\n","for layer in base_model.layers:\n","    layer.trainable = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compile the model\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the model\n","hist = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), batch_size=45)  # Specify batch size here"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the model\n","model.save(\"drowsiness_inceptionv4.h5\")\n","model.save(\"drowsiness_inceptionv4.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot accuracy and loss\n","accuracy = hist.history['accuracy']\n","val_accuracy = hist.history['val_accuracy']\n","loss = hist.history['loss']\n","val_loss = hist.history['val_loss']\n","epochs = range(len(accuracy))\n","\n","plt.plot(epochs, accuracy, \"b\", label=\"training accuracy\")\n","plt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Training & Validation Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(epochs, loss, \"b\", label=\"training loss\")\n","plt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training & Validation Loss')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions and evaluate the model\n","y_true = np.argmax(y_test, axis=1)\n","y_pred = model.predict(x_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Confusion Matrix\n","confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n","\n","# Plot Confusion Matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Print the confusion matrix\n","print(\"Confusion Matrix:\\n\", confusion_mtx)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Accuracy Score\n","accuracy = np.sum(y_true == y_pred_classes) / len(y_true)\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Classification Report\n","class_report = classification_report(y_true, y_pred_classes, target_names=labels)\n","print(\"Classification Report:\\n\", class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compute ROC curves and AUC for each class\n","n_classes = 4\n","fpr = dict()\n","tpr = dict()\n","\n","roc_auc = dict()\n","\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Plot the ROC curves for each class\n","plt.figure(figsize=(8, 6))\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i], lw=2, label='Class {} (AUC = {:.2f})'.format(i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
